# -*- coding: utf-8 -*-
"""Untitled33.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rpKvqBbnU9gClW02znK4tm4HtukGwHRw
"""

# Install dependencies
import subprocess
import sys

packages = ['yfinance', 'backtrader', 'quantstats', 'ta']
for package in packages:
    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])

# =============================================================================
# IMPORTS
# =============================================================================
import os
import warnings
import numpy as np
import pandas as pd
import yfinance as yf
import backtrader as bt
import quantstats as qs
import ta
from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_score, roc_auc_score

warnings.filterwarnings('ignore')

# =============================================================================
# CONFIGURATION
# =============================================================================
SMALL_UNIVERSE = ['FIX', 'TSLA', 'CNP', 'DLTR', 'WMS', 'HAS', 'HIBB', 'RHI', 'TGT', 'WBA']
LARGE_UNIVERSE = [
    'FIX', 'TSLA', 'CNP', 'DLTR', 'WMS', 'HAS', 'RHI', 'TGT',
    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'NVDA', 'JPM', 'BAC',
    'WFC', 'GS', 'MS', 'V', 'MA', 'PYPL', 'DIS', 'NFLX', 'CMCSA',
    'T', 'VZ', 'PFE', 'JNJ', 'UNH', 'MRK', 'ABBV', 'CVX', 'XOM',
    'COP', 'SLB', 'HD', 'LOW', 'TJX', 'ROST', 'NKE', 'SBUX', 'MCD',
    'YUM', 'CMG', 'COST', 'WMT', 'KR', 'CAT', 'DE', 'MMM', 'HON',
    'GE', 'BA', 'LMT', 'RTX', 'F', 'GM'
]

START_DATE = '2000-01-01'
END_DATE = '2021-11-12'
TRAIN_RATIO = 0.6
INITIAL_CASH = 100000
COMMISSION = 0.001

OUTPUT_DIR = '/content/final_outputs'
BACKTEST_DIR = '/content/backtest_data'
REPORTS_DIR = '/content/trading_reports'

for dir_path in [OUTPUT_DIR, BACKTEST_DIR, REPORTS_DIR]:
    os.makedirs(dir_path, exist_ok=True)

# =============================================================================
# CLASS DEFINITIONS
# =============================================================================

# Data Preprocessing
class DataPreprocessor:
    """Handles data cleaning, missing value imputation, and outlier removal."""
    def __init__(self, df, ticker):
        self.df = df.copy()
        self.ticker = ticker

    def handle_missing_data(self):
        """Fill missing data using forward fill."""
        self.df = self.df.sort_values('date').reset_index(drop=True)
        price_cols = ['open', 'high', 'low', 'close']
        self.df[price_cols] = self.df[price_cols].ffill()
        self.df['volume'] = self.df['volume'].ffill().fillna(0)
        return self

    def remove_outliers(self, n_std=5):
        """Remove extreme outliers using z-score method."""
        price_cols = ['open', 'high', 'low', 'close']
        for col in price_cols:
            mean = self.df[col].mean()
            std = self.df[col].std()
            lower_bound = mean - n_std * std
            upper_bound = mean + n_std * std
            outlier_mask = (self.df[col] < lower_bound) | (self.df[col] > upper_bound)
            if outlier_mask.sum() > 0:
                self.df.loc[outlier_mask, col] = np.nan
                self.df[col] = self.df[col].ffill().bfill()
        return self

    def validate_data(self):
        """Validate data integrity."""
        self.df['high'] = self.df[['open', 'high', 'low', 'close']].max(axis=1)
        self.df['low'] = self.df[['open', 'high', 'low', 'close']].min(axis=1)
        return self

    def get_processed_data(self):
        return self.df

# Feature Engineering
class FeatureEngineer:
    """Creates features for stock price direction prediction."""
    def __init__(self, df):
        self.df = df.copy()
        self.df = self.df.sort_values('date').reset_index(drop=True)

    def add_returns(self):
        """Calculate return measures and target variable."""
        self.df['returns'] = self.df['close'].pct_change()
        self.df['log_returns'] = np.log(self.df['close'] / self.df['close'].shift(1))
        self.df['future_return'] = self.df['returns'].shift(-1)
        self.df['target'] = (self.df['future_return'] > 0).astype(int)
        return self

    def add_moving_averages(self):
        """Add moving averages with different window sizes."""
        windows = [5, 10, 20, 50, 100, 200]
        for window in windows:
            self.df[f'sma_{window}'] = self.df['close'].rolling(window=window).mean()
            self.df[f'ema_{window}'] = self.df['close'].ewm(span=window, adjust=False).mean()
            self.df[f'close_to_sma_{window}'] = self.df['close'] / self.df[f'sma_{window}'] - 1
        self.df['sma_5_20_cross'] = (self.df['sma_5'] > self.df['sma_20']).astype(int)
        self.df['sma_20_50_cross'] = (self.df['sma_20'] > self.df['sma_50']).astype(int)
        return self

    def add_momentum_indicators(self):
        """Add momentum-based technical indicators."""
        self.df['rsi_14'] = ta.momentum.RSIIndicator(self.df['close'], window=14).rsi()
        self.df['rsi_7'] = ta.momentum.RSIIndicator(self.df['close'], window=7).rsi()
        macd = ta.trend.MACD(self.df['close'])
        self.df['macd'] = macd.macd()
        self.df['macd_signal'] = macd.macd_signal()
        self.df['macd_diff'] = macd.macd_diff()
        stoch = ta.momentum.StochasticOscillator(self.df['high'], self.df['low'], self.df['close'])
        self.df['stoch_k'] = stoch.stoch()
        self.df['stoch_d'] = stoch.stoch_signal()
        self.df['roc_5'] = ta.momentum.ROCIndicator(self.df['close'], window=5).roc()
        self.df['roc_10'] = ta.momentum.ROCIndicator(self.df['close'], window=10).roc()
        return self

    def add_volatility_indicators(self):
        """Add volatility-based indicators."""
        bb = ta.volatility.BollingerBands(self.df['close'], window=20, window_dev=2)
        self.df['bb_high'] = bb.bollinger_hband()
        self.df['bb_low'] = bb.bollinger_lband()
        self.df['bb_mid'] = bb.bollinger_mavg()
        self.df['bb_width'] = (self.df['bb_high'] - self.df['bb_low']) / self.df['bb_mid']
        self.df['bb_pct'] = bb.bollinger_pband()
        self.df['atr_14'] = ta.volatility.AverageTrueRange(
            self.df['high'], self.df['low'], self.df['close'], window=14
        ).average_true_range()
        self.df['volatility_20'] = self.df['returns'].rolling(window=20).std() * np.sqrt(252)
        return self

    def add_volume_indicators(self):
        """Add volume-based indicators."""
        self.df['volume_sma_20'] = self.df['volume'].rolling(window=20).mean()
        self.df['volume_ratio'] = self.df['volume'] / self.df['volume_sma_20']
        self.df['obv'] = ta.volume.OnBalanceVolumeIndicator(
            self.df['close'], self.df['volume']
        ).on_balance_volume()
        self.df['vpt'] = ta.volume.VolumePriceTrendIndicator(
            self.df['close'], self.df['volume']
        ).volume_price_trend()
        return self

    def add_price_patterns(self):
        """Add price pattern features."""
        self.df['body'] = self.df['close'] - self.df['open']
        self.df['body_pct'] = self.df['body'] / self.df['open']
        self.df['upper_shadow'] = self.df['high'] - self.df[['open', 'close']].max(axis=1)
        self.df['lower_shadow'] = self.df[['open', 'close']].min(axis=1) - self.df['low']
        self.df['daily_range'] = (self.df['high'] - self.df['low']) / self.df['low']
        self.df['gap'] = self.df['open'] / self.df['close'].shift(1) - 1
        return self

    def add_lagged_features(self):
        """Add lagged versions of key features."""
        lag_features = ['returns', 'volume_ratio', 'rsi_14']
        lags = [1, 2, 3, 5]
        for feature in lag_features:
            if feature in self.df.columns:
                for lag in lags:
                    self.df[f'{feature}_lag_{lag}'] = self.df[feature].shift(lag)
        return self

    def create_all_features(self):
        """Create all features."""
        self = (self.add_returns()
                .add_moving_averages()
                .add_momentum_indicators()
                .add_volatility_indicators()
                .add_volume_indicators()
                .add_price_patterns()
                .add_lagged_features())
        return self.df

# ML Data Preparation
class MLDataPreparer:
    """Prepares data for machine learning models."""
    def __init__(self, df, ticker):
        self.df = df.copy()
        self.ticker = ticker
        self.scaler = StandardScaler()
        self.feature_columns = None

    def prepare_features(self):
        """Select feature columns."""
        exclude_cols = ['date', 'open', 'high', 'low', 'close', 'volume',
                        'future_return', 'target', 'returns', 'log_returns',
                        'obv', 'vpt']
        self.feature_columns = [col for col in self.df.columns if col not in exclude_cols]
        return self

    def handle_missing_values(self):
        """Drop rows with missing values."""
        cols_to_check = self.feature_columns + ['target']
        self.df = self.df.dropna(subset=cols_to_check)
        self.df = self.df[self.df['future_return'].notna()]
        return self

    def split_data(self, train_ratio=0.6):
        """Split data chronologically."""
        n = len(self.df)
        train_size = int(n * train_ratio)
        train_df = self.df.iloc[:train_size].copy()
        test_df = self.df.iloc[train_size:].copy()
        return train_df, test_df

    def normalize_features(self, train_df, test_df):
        """Normalize features using StandardScaler."""
        X_train = train_df[self.feature_columns].values
        X_test = test_df[self.feature_columns].values
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        y_train = train_df['target'].values
        y_test = test_df['target'].values
        train_dates = train_df['date'].values
        test_dates = test_df['date'].values
        return (X_train_scaled, X_test_scaled, y_train, y_test, train_dates, test_dates)

    def prepare(self, train_ratio=0.6):
        """Complete preparation pipeline."""
        self.prepare_features()
        self.handle_missing_values()
        train_df, test_df = self.split_data(train_ratio)
        return self.normalize_features(train_df, test_df)

# Model Training
class ModelTrainer:
    """Trains and evaluates classification models."""
    def __init__(self, X_train, X_test, y_train, y_test, ticker):
        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.y_test = y_test
        self.ticker = ticker
        self.models = {}
        self.best_params = {}
        self.predictions = {}
        self.metrics = {}

    def train_random_forest(self):
        """Train Random Forest with hyperparameter tuning."""
        param_grid = {
            'n_estimators': [100, 200],
            'max_depth': [5, 10, 20, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        tscv = TimeSeriesSplit(n_splits=3)
        rf = RandomForestClassifier(random_state=42, n_jobs=-1)
        grid_search = GridSearchCV(rf, param_grid, cv=tscv, scoring='accuracy',
                                   n_jobs=-1, verbose=0)
        grid_search.fit(self.X_train, self.y_train)
        self.models['RandomForest'] = grid_search.best_estimator_
        self.best_params['RandomForest'] = grid_search.best_params_
        return self

    def train_logistic_regression(self):
        """Train Logistic Regression with hyperparameter tuning."""
        param_grid = {
            'C': [0.001, 0.01, 0.1, 1, 10],
            'penalty': ['l1', 'l2'],
            'solver': ['saga'],
            'max_iter': [1000]
        }
        tscv = TimeSeriesSplit(n_splits=3)
        lr = LogisticRegression(random_state=42)
        grid_search = GridSearchCV(lr, param_grid, cv=tscv, scoring='accuracy',
                                   n_jobs=-1, verbose=0)
        grid_search.fit(self.X_train, self.y_train)
        self.models['LogisticRegression'] = grid_search.best_estimator_
        self.best_params['LogisticRegression'] = grid_search.best_params_
        return self

    def evaluate_models(self):
        """Evaluate models on test data."""
        for model_name, model in self.models.items():
            y_pred = model.predict(self.X_test)
            y_pred_proba = model.predict_proba(self.X_test)[:, 1]
            accuracy = accuracy_score(self.y_test, y_pred)
            precision = precision_score(self.y_test, y_pred, zero_division=0)
            try:
                roc_auc = roc_auc_score(self.y_test, y_pred_proba)
            except:
                roc_auc = 0.5
            self.predictions[model_name] = y_pred
            self.metrics[model_name] = {
                'accuracy': accuracy,
                'precision': precision,
                'roc_auc': roc_auc
            }
        return self

    def get_results(self):
        return {
            'models': self.models,
            'best_params': self.best_params,
            'predictions': self.predictions,
            'metrics': self.metrics
        }

    def train_all(self):
        """Complete training pipeline."""
        self.train_random_forest()
        self.train_logistic_regression()
        self.evaluate_models()
        return self.get_results()

# BackTrader Classes
class CustomCSVData(bt.feeds.GenericCSVData):
    """Custom data class for BackTrader supporting ML signals."""
    lines = ('signal', 'probability',)
    params = (
        ('format_type', 1),
        ('datetime', 0),
        ('open', 1),
        ('high', 2),
        ('low', 3),
        ('close', 4),
        ('volume', 5),
        ('openinterest', -1),
        ('signal', -1),
        ('probability', -1),
        ('dtformat', '%Y-%m-%d'),
    )

class MLStrategy(bt.Strategy):
    """ML-based trading strategy."""
    params = (
        ('stake_pct', 0.95),
        ('printlog', False),
    )

    def __init__(self):
        self.order = None
        self.signal = self.datas[0].signal
        self.trades = []
        self.trade_count = 0
        self.win_count = 0

    def notify_order(self, order):
        if order.status in [order.Completed]:
            pass
        self.order = None

    def notify_trade(self, trade):
        if trade.isclosed:
            self.trade_count += 1
            if trade.pnl > 0:
                self.win_count += 1

    def next(self):
        if self.order:
            return
        current_signal = self.signal[0]
        if not self.position:
            if current_signal == 1:
                cash = self.broker.getcash()
                price = self.datas[0].close[0]
                size = int((cash * self.params.stake_pct) / price)
                if size > 0:
                    self.order = self.buy(size=size)
        else:
            if current_signal == 0:
                self.order = self.sell(size=self.position.size)

class BacktestRunner:
    """Runs backtests and generates performance reports."""
    def __init__(self, initial_cash=100000, commission=0.001):
        self.initial_cash = initial_cash
        self.commission = commission
        self.results = {}

    def run_backtest(self, ticker, model_name, data_path):
        """Run a single backtest."""
        cerebro = bt.Cerebro()
        cerebro.addstrategy(MLStrategy, printlog=False)
        data = CustomCSVData(
            dataname=data_path,
            format_type=3,
            fromdate=datetime(2000, 1, 1),
            todate=datetime(2021, 11, 12),
            signal=6,
            probability=7
        )
        cerebro.adddata(data)
        cerebro.broker.setcash(self.initial_cash)
        cerebro.broker.setcommission(commission=self.commission)
        cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe',
                          riskfreerate=0.02, annualize=True)
        cerebro.addanalyzer(bt.analyzers.DrawDown, _name='drawdown')
        cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trades')
        cerebro.addanalyzer(bt.analyzers.TimeReturn, _name='timereturn')

        results = cerebro.run()
        strat = results[0]
        final_value = cerebro.broker.getvalue()
        total_return = (final_value - self.initial_cash) / self.initial_cash * 100

        sharpe = strat.analyzers.sharpe.get_analysis()
        drawdown = strat.analyzers.drawdown.get_analysis()
        trades = strat.analyzers.trades.get_analysis()

        sharpe_ratio = sharpe.get('sharperatio', 0) or 0
        max_drawdown = drawdown.get('max', {}).get('drawdown', 0) or 0
        total_trades = trades.get('total', {}).get('total', 0) or 0
        won_trades = trades.get('won', {}).get('total', 0) or 0
        win_rate = (won_trades / total_trades * 100) if total_trades > 0 else 0

        time_returns = strat.analyzers.timereturn.get_analysis()
        returns_series = pd.Series(time_returns)
        returns_series.index = pd.to_datetime(returns_series.index)

        return {
            'ticker': ticker,
            'model': model_name,
            'initial_value': self.initial_cash,
            'final_value': final_value,
            'total_return': total_return,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'total_trades': total_trades,
            'win_rate': win_rate,
            'returns_series': returns_series
        }

    def generate_report(self, result, output_dir):
        """Generate quantstats HTML report."""
        ticker = result['ticker']
        model = result['model']
        returns = result['returns_series']
        if len(returns) > 0:
            filename = f'{ticker}_{model}_report.html'
            filepath = os.path.join(output_dir, filename)
            try:
                qs.reports.html(returns, output=filepath,
                               title=f'{ticker} - {model} Strategy Report')
                return filepath
            except:
                return None
        return None

# =============================================================================
# HELPER FUNCTIONS
# =============================================================================

def download_data(tickers, start_date, end_date):
    """Download stock data from Yahoo Finance."""
    stock_data = {}
    for ticker in tickers:
        try:
            df = yf.download(ticker, start=start_date, end=end_date, progress=False)
            if len(df) > 0:
                df = df.reset_index()
                df.columns = [col.lower() if isinstance(col, str) else col[0].lower()
                             for col in df.columns]
                stock_data[ticker] = df
        except:
            pass
    return stock_data

def process_stock(ticker, df):
    """Process a single stock through the complete pipeline."""
    preprocessor = DataPreprocessor(df, ticker)
    processed_df = (preprocessor.handle_missing_data()
                    .remove_outliers()
                    .validate_data()
                    .get_processed_data())
    engineer = FeatureEngineer(processed_df)
    featured_df = engineer.create_all_features()
    preparer = MLDataPreparer(featured_df, ticker)
    return preparer.prepare(), processed_df

def create_backtest_file(ticker, model_name, processed_df, predictions, probabilities, test_dates):
    """Create backtest data file with ML signals."""
    orig_df = processed_df.copy()
    orig_df['date'] = pd.to_datetime(orig_df['date'])
    pred_df = pd.DataFrame({
        'date': pd.to_datetime(test_dates),
        'signal': predictions,
        'probability': probabilities
    })
    merged_df = orig_df.merge(pred_df, on='date', how='left')
    merged_df['signal'] = merged_df['signal'].fillna(0).astype(int)
    merged_df['probability'] = merged_df['probability'].fillna(0.5)
    bt_df = merged_df[['date', 'open', 'high', 'low', 'close', 'volume',
                       'signal', 'probability']].copy()
    bt_df['date'] = bt_df['date'].dt.strftime('%Y-%m-%d')
    bt_df = bt_df.sort_values('date')
    filepath = os.path.join(BACKTEST_DIR, f'{ticker}_{model_name}.csv')
    bt_df.to_csv(filepath, index=False)
    return filepath

# =============================================================================
# MAIN EXECUTION - SMALL UNIVERSE
# =============================================================================

stock_data = download_data(SMALL_UNIVERSE, START_DATE, END_DATE)
available_tickers = list(stock_data.keys())

small_results = []
backtest_results = []
runner = BacktestRunner(INITIAL_CASH, COMMISSION)

for ticker in available_tickers:
    try:
        (X_train, X_test, y_train, y_test, train_dates, test_dates), processed_df = process_stock(
            ticker, stock_data[ticker]
        )

        if len(X_train) < 100 or len(X_test) < 50:
            continue

        trainer = ModelTrainer(X_train, X_test, y_train, y_test, ticker)
        results = trainer.train_all()

        for model_name, metrics in results['metrics'].items():
            small_results.append({
                'Ticker': ticker,
                'Model': model_name,
                'Accuracy': metrics['accuracy'],
                'Precision': metrics['precision'],
                'ROC_AUC': metrics['roc_auc']
            })

            predictions = results['predictions'][model_name]
            model = results['models'][model_name]
            probabilities = model.predict_proba(X_test)[:, 1]

            filepath = create_backtest_file(ticker, model_name, processed_df,
                                          predictions, probabilities, test_dates)

            try:
                bt_result = runner.run_backtest(ticker, model_name, filepath)
                backtest_results.append({
                    'Ticker': ticker,
                    'Model': model_name,
                    'Final_Value': bt_result['final_value'],
                    'Return_Pct': bt_result['total_return'],
                    'Sharpe_Ratio': bt_result['sharpe_ratio'],
                    'Max_Drawdown': bt_result['max_drawdown'],
                    'Total_Trades': bt_result['total_trades'],
                    'Win_Rate': bt_result['win_rate']
                })
                runner.generate_report(bt_result, REPORTS_DIR)
            except:
                pass
    except:
        pass

small_df = pd.DataFrame(small_results)
small_df.to_csv(os.path.join(OUTPUT_DIR, 'small_universe_results.csv'), index=False)

backtest_df = pd.DataFrame(backtest_results)
backtest_df.to_csv(os.path.join(OUTPUT_DIR, 'small_universe_backtest.csv'), index=False)

# =============================================================================
# MAIN EXECUTION - LARGE UNIVERSE
# =============================================================================

large_stock_data = download_data(LARGE_UNIVERSE, START_DATE, END_DATE)
large_results = []
top_10_backtest = []

for ticker in large_stock_data.keys():
    try:
        (X_train, X_test, y_train, y_test, train_dates, test_dates), processed_df = process_stock(
            ticker, large_stock_data[ticker]
        )

        if len(X_train) < 100 or len(X_test) < 50:
            continue

        rf = RandomForestClassifier(n_estimators=100, max_depth=10,
                                   min_samples_split=5, random_state=42, n_jobs=-1)
        rf.fit(X_train, y_train)
        rf_pred = rf.predict(X_test)
        rf_acc = accuracy_score(y_test, rf_pred)
        rf_prec = precision_score(y_test, rf_pred, zero_division=0)

        lr = LogisticRegression(C=1.0, max_iter=1000, random_state=42)
        lr.fit(X_train, y_train)
        lr_pred = lr.predict(X_test)
        lr_acc = accuracy_score(y_test, lr_pred)
        lr_prec = precision_score(y_test, lr_pred, zero_division=0)

        large_results.append({
            'Ticker': ticker,
            'RF_Accuracy': rf_acc,
            'RF_Precision': rf_prec,
            'LR_Accuracy': lr_acc,
            'LR_Precision': lr_prec,
            'Best_Accuracy': max(rf_acc, lr_acc),
            'Best_Model': 'RandomForest' if rf_acc > lr_acc else 'LogisticRegression',
            'Train_Samples': len(X_train),
            'Test_Samples': len(X_test)
        })
    except:
        pass

large_df = pd.DataFrame(large_results)
large_df.to_csv(os.path.join(OUTPUT_DIR, 'large_universe_results.csv'), index=False)

top_10 = large_df.nlargest(10, 'Best_Accuracy')

for _, row in top_10.iterrows():
    ticker = row['Ticker']
    best_model = row['Best_Model']

    try:
        (X_train, X_test, y_train, y_test, train_dates, test_dates), processed_df = process_stock(
            ticker, large_stock_data[ticker]
        )

        if best_model == 'RandomForest':
            model = RandomForestClassifier(n_estimators=100, max_depth=10,
                                          min_samples_split=5, random_state=42)
        else:
            model = LogisticRegression(C=1.0, max_iter=1000, random_state=42)

        model.fit(X_train, y_train)
        predictions = model.predict(X_test)
        probabilities = model.predict_proba(X_test)[:, 1]

        filepath = create_backtest_file(ticker, best_model, processed_df,
                                       predictions, probabilities, test_dates)

        bt_result = runner.run_backtest(ticker, best_model, filepath)
        top_10_backtest.append({
            'Ticker': ticker,
            'Model': best_model,
            'Accuracy': row['Best_Accuracy'],
            'Precision': row[f'{best_model[:2]}_Precision'],
            'Final_Value': bt_result['final_value'],
            'Return_Pct': bt_result['total_return'],
            'Sharpe_Ratio': bt_result['sharpe_ratio'],
            'Max_Drawdown': bt_result['max_drawdown'],
            'Total_Trades': bt_result['total_trades'],
            'Win_Rate': bt_result['win_rate']
        })
        runner.generate_report(bt_result, REPORTS_DIR)
    except:
        pass

top_10_df = pd.DataFrame(top_10_backtest)
top_10_df.to_csv(os.path.join(OUTPUT_DIR, 'top_10_stocks_results.csv'), index=False)

# =============================================================================
# GENERATE SUBMISSION FILES
# =============================================================================


# Generate README
# readme_content = '''================================================================================
# ISYE 6767 - Project 2: Data Analysis and Machine Learning
# README
# ================================================================================

# PYTHON VERSION AND DEPENDENCIES
# -------------------------------
# Python Version: 3.7.x

# Required Packages:
# - pandas >= 1.3.0
# - numpy >= 1.21.0
# - scikit-learn >= 0.24.0
# - yfinance >= 0.1.70
# - backtrader >= 1.9.76
# - quantstats >= 0.0.59
# - ta >= 0.10.0

# Install dependencies:
#     pip install pandas numpy scikit-learn yfinance backtrader quantstats ta

# EXECUTION INSTRUCTIONS
# ----------------------
# Run the main script:
#     python LastName_FirstName_project2.py

# OUTPUT FILES
# ------------
# 1. small_universe_results.csv - ML metrics for small universe
# 2. small_universe_backtest.csv - Backtest metrics for small universe
# 3. large_universe_results.csv - ML metrics for large universe
# 4. top_10_stocks_results.csv - Top 10 stocks with backtest results

# All outputs are saved to the 'final_outputs' directory.
# Trading reports (HTML) are saved to the 'trading_reports' directory.

# NOTES
# -----
# - HIBB and WBA tickers were excluded due to data availability issues
# - Models are trained separately for each stock
# - 60/40 train/test split is used (chronological)
# - TimeSeriesSplit cross-validation respects temporal order
# ================================================================================
# '''

# with open('/content/LastName_FirstName_README.txt', 'w') as f:
#     f.write(readme_content)

# # =============================================================================
# # FINAL OUTPUT SUMMARY
# # =============================================================================

# print("="*70)
# print("PROJECT 2 - EXECUTION COMPLETE")
# print("="*70)
# print(f"\nSmall Universe:")
# print(f"  - Tickers processed: {len(available_tickers)}")
# print(f"  - Models trained: {len(small_results)}")
# print(f"  - Backtests completed: {len(backtest_results)}")
# print(f"\nLarge Universe:")
# print(f"  - Stocks processed: {len(large_results)}")
# print(f"  - Top 10 backtests: {len(top_10_backtest)}")
# print(f"\nOutput Files:")
# print(f"  - {OUTPUT_DIR}/small_universe_results.csv")
# print(f"  - {OUTPUT_DIR}/small_universe_backtest.csv")
# print(f"  - {OUTPUT_DIR}/large_universe_results.csv")
# print(f"  - {OUTPUT_DIR}/top_10_stocks_results.csv")
# print(f"  - {len(os.listdir(REPORTS_DIR))} HTML reports in {REPORTS_DIR}/")
# print(f"\nSubmission Files:")
# print(f"  - /content/LastName_FirstName_project2.py")
# print(f"  - /content/LastName_FirstName_README.txt")
# print("\n" + "="*70)
# print("CODING PART COMPLETE - Ready for Report Writing")
# print("="*70)

# Take the 10 stocks with highest Best_Accuracy
top_10_from_large = large_df.nlargest(10, 'Best_Accuracy')[
    ['Ticker', 'Best_Model', 'Best_Accuracy', 'RF_Accuracy', 'LR_Accuracy']
].reset_index(drop=True)

# Save to CSV
top_10_from_large.to_csv(
    os.path.join(OUTPUT_DIR, 'large_universe_top10_by_accuracy.csv'),
    index=False
)